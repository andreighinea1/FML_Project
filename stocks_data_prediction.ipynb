{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "640a2e7cf6e8d37a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b18855ea",
   "metadata": {
    "papermill": {
     "duration": 0.003065,
     "end_time": "2024-12-20T22:32:34.882291",
     "exception": false,
     "start_time": "2024-12-20T22:32:34.879226",
     "status": "completed"
    },
    "tags": []
   },
   "source": "# Data Loading"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Load a DataFrame with a specific version of a CSV\n",
    "df: pd.DataFrame = kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"shiveshprakash/34-year-daily-stock-data/versions/1\",\n",
    "    \"stock_data.csv\",\n",
    ")\n",
    "\n",
    "# Drop useless columns or which we will create ourselves\n",
    "df = df.drop(columns=[\"prev_day\"])\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ],
   "id": "88b04371e8b9dd55",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "271de872",
   "metadata": {
    "papermill": {
     "duration": 0.003422,
     "end_time": "2024-12-20T22:32:36.896597",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.893175",
     "status": "completed"
    },
    "tags": []
   },
   "source": "# Data Cleaning and Preprocessing"
  },
  {
   "cell_type": "code",
   "id": "32f8eb5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:36.905286Z",
     "iopub.status.busy": "2024-12-20T22:32:36.904900Z",
     "iopub.status.idle": "2024-12-20T22:32:36.915093Z",
     "shell.execute_reply": "2024-12-20T22:32:36.914061Z"
    },
    "id": "data-cleaning",
    "papermill": {
     "duration": 0.016478,
     "end_time": "2024-12-20T22:32:36.916812",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.900334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e055ca0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:36.925869Z",
     "iopub.status.busy": "2024-12-20T22:32:36.925530Z",
     "iopub.status.idle": "2024-12-20T22:32:36.938118Z",
     "shell.execute_reply": "2024-12-20T22:32:36.937196Z"
    },
    "id": "data-preprocessing",
    "papermill": {
     "duration": 0.019021,
     "end_time": "2024-12-20T22:32:36.939800",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.920779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convert \"dt\" to datetime format\n",
    "df[\"dt\"] = pd.to_datetime(df[\"dt\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Check data types\n",
    "df.dtypes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa788377",
   "metadata": {
    "papermill": {
     "duration": 0.003836,
     "end_time": "2024-12-20T22:32:36.947821",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.943985",
     "status": "completed"
    },
    "tags": []
   },
   "source": "# Data Analysis"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### General Plots",
   "id": "25cc0f0cd6045363"
  },
  {
   "cell_type": "code",
   "id": "15b99a7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:36.957262Z",
     "iopub.status.busy": "2024-12-20T22:32:36.956917Z",
     "iopub.status.idle": "2024-12-20T22:32:37.395839Z",
     "shell.execute_reply": "2024-12-20T22:32:37.394832Z"
    },
    "id": "eda",
    "papermill": {
     "duration": 0.445954,
     "end_time": "2024-12-20T22:32:37.397827",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.951873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plot the S&P 500 over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=df[\"dt\"], y=df[\"sp500\"], label=\"S&P 500\")\n",
    "plt.title(\"S&P 500 Index Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"S&P 500 Index\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f06cc25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:37.409048Z",
     "iopub.status.busy": "2024-12-20T22:32:37.408621Z",
     "iopub.status.idle": "2024-12-20T22:32:37.690464Z",
     "shell.execute_reply": "2024-12-20T22:32:37.689398Z"
    },
    "id": "eda-2",
    "papermill": {
     "duration": 0.289506,
     "end_time": "2024-12-20T22:32:37.692271",
     "exception": false,
     "start_time": "2024-12-20T22:32:37.402765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualize the relationship between S&P 500 and DJIA\n",
    "sns.scatterplot(x=\"sp500\", y=\"djia\", data=df)\n",
    "plt.title(\"S&P 500 vs DJIA\")\n",
    "plt.xlabel(\"S&P 500 Index\")\n",
    "plt.ylabel(\"DJIA Index\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab5b2c71",
   "metadata": {
    "papermill": {
     "duration": 0.00554,
     "end_time": "2024-12-20T22:32:37.705110",
     "exception": false,
     "start_time": "2024-12-20T22:32:37.699570",
     "status": "completed"
    },
    "tags": []
   },
   "source": "### Correlation Analysis"
  },
  {
   "cell_type": "code",
   "id": "1651eb6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:37.718150Z",
     "iopub.status.busy": "2024-12-20T22:32:37.717809Z",
     "iopub.status.idle": "2024-12-20T22:32:38.385552Z",
     "shell.execute_reply": "2024-12-20T22:32:38.384335Z"
    },
    "id": "correlation-analysis",
    "papermill": {
     "duration": 0.677953,
     "end_time": "2024-12-20T22:32:38.389005",
     "exception": false,
     "start_time": "2024-12-20T22:32:37.711052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Select only numeric columns for correlation analysis\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create new features",
   "id": "3eec01b82184f2f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Aggregate Rolling Features",
   "id": "94ab71317a25c398"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "windows = [7, 14, 30, 90, 365]  # Rolling window sizes (days)\n",
    "\n",
    "cols_to_aggregate = [\"sp500\", \"sp500_volume\", \"djia\", \"djia_volume\", \"hsi\", \"vix\"]\n",
    "less_correlated_features = [\"us3m\", \"joblessness\", \"epu\"]\n",
    "cols_to_aggregate_all = cols_to_aggregate + less_correlated_features\n",
    "nr_window_features = 0\n",
    "for window in windows:\n",
    "    for col in cols_to_aggregate_all:\n",
    "        df[f\"{col}_mean_{window}\"] = df[col].rolling(window=window, min_periods=1).mean()\n",
    "        df[f\"{col}_std_{window}\"] = df[col].rolling(window=window, min_periods=1).std()\n",
    "        nr_window_features += 2\n",
    "\n",
    "# Drop rows with NaN values introduced by rolling calculations\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Nr of window features: {nr_window_features}\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()"
   ],
   "id": "6bd47f79caf76ccd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Autoencoders",
   "id": "21535c3d73c557af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create lagged features (temporary DataFrame)\n",
    "lag_days = 365\n",
    "lagged_df = pd.DataFrame()\n",
    "# for lag in range(1, lag_days + 1):\n",
    "# lagged_df[f\"sp500_lag_{lag}\"] = df[\"sp500\"].shift(lag)\n",
    "# lagged_df[f\"sp500_volume_lag_{lag}\"] = df[\"sp500_volume\"].shift(lag)\n",
    "# lagged_df[f\"djia_lag_{lag}\"] = df[\"djia\"].shift(lag)\n",
    "# lagged_df[f\"djia_volume_lag_{lag}\"] = df[\"djia_volume\"].shift(lag)\n",
    "# lagged_df[f\"hsi_lag_{lag}\"] = df[\"hsi\"].shift(lag)\n",
    "# lagged_df[f\"vix_lag_{lag}\"] = df[\"vix\"].shift(lag)\n",
    "\n",
    "for col in cols_to_aggregate:\n",
    "    for lag in range(1, lag_days + 1):\n",
    "        lagged_df[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
    "for col in less_correlated_features:\n",
    "    for lag in [7, 14, 30, 90]:\n",
    "        lagged_df[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "lagged_df = lagged_df.dropna()\n",
    "\n",
    "# Normalize lagged features\n",
    "scaler = MinMaxScaler()\n",
    "X_lagged = scaler.fit_transform(lagged_df.values)\n",
    "joblib.dump(scaler, \"lagged_scaler.pkl\")  # Save the scaler\n",
    "\n",
    "# Clean up: Delete the temporary lagged features DataFrame\n",
    "del lagged_df\n",
    "\n",
    "# Get input dimensions\n",
    "input_dim = X_lagged.shape[1]  # Number of lagged features\n",
    "\n",
    "# Display the number of lagged features\n",
    "print(f\"Nr of lagged features: {input_dim}\")"
   ],
   "id": "72146f3e514985e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "from backend.autoencoders import objective\n",
    "\n",
    "# Number of trials for optimization\n",
    "n_trials = 100\n",
    "\n",
    "# Run Optuna optimization\n",
    "study_ae = optuna.create_study(direction=\"minimize\")\n",
    "study_ae.optimize(lambda trial: objective(trial, X_lagged), n_trials=n_trials)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"\\nBest Autoencoder Parameters:\")\n",
    "print(study_ae.best_params)"
   ],
   "id": "f05069458103e8f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from backend.autoencoders import Autoencoder, train_predict_autoencoder\n",
    "\n",
    "# Get best parameters\n",
    "best_params = study_ae.best_params\n",
    "\n",
    "# Initialize Autoencoder with best parameters\n",
    "autoencoder = Autoencoder(\n",
    "    input_dim=X_lagged.shape[1],\n",
    "    encoding_dim=best_params[\"encoding_dim\"],\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    dropout_rate=best_params[\"dropout_rate\"],\n",
    ")\n",
    "\n",
    "# Train Autoencoder\n",
    "trained_autoencoder, embeddings, last_mse_loss = train_predict_autoencoder(\n",
    "    autoencoder,\n",
    "    X_lagged,\n",
    "    epochs=150,\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    lr=best_params[\"lr\"],\n",
    "    l1_penalty=best_params[\"l1_penalty\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    ")"
   ],
   "id": "b37ef7f2808347bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert embeddings to DataFrame\n",
    "embedding_df = pd.DataFrame(embeddings, columns=[f\"embed_{i + 1}\" for i in range(embeddings.shape[1])])"
   ],
   "id": "55e953d8f57ab8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare the final df to train",
   "id": "13f6e0affe261b19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Take the right data for training",
   "id": "3df0dd01ab1a4b87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select only numeric columns for training\n",
    "training_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Convert to an ordered categorical column\n",
    "if training_df[\"joblessness\"].dtypes != \"category\":\n",
    "    training_df[\"joblessness\"] = pd.Categorical(\n",
    "        training_df[\"joblessness\"],\n",
    "        categories=[1, 2, 3, 4],\n",
    "        ordered=True\n",
    "    )"
   ],
   "id": "2c360326d9e64aa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Scale the df",
   "id": "b93e6935118fab42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Separate the \"joblessness\" column\n",
    "joblessness = training_df[\"joblessness\"]\n",
    "\n",
    "# Select all columns except \"joblessness\"\n",
    "columns_to_scale = training_df.drop(columns=[\"joblessness\"]).columns\n",
    "\n",
    "# Apply MinMaxScaler to the selected columns\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(training_df[columns_to_scale])\n",
    "joblib.dump(scaler, \"training_df_scaler.pkl\")  # Save the scaler\n",
    "\n",
    "# Create a DataFrame for the scaled data\n",
    "training_df = pd.DataFrame(scaled_data, columns=columns_to_scale, index=training_df.index)\n",
    "\n",
    "# Add back the \"joblessness\" column\n",
    "training_df[\"joblessness\"] = joblessness"
   ],
   "id": "44043096e505e76b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Attach embeddings to the training DataFrame",
   "id": "4a63be8a5c808bb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Attach embeddings to the main DataFrame\n",
    "training_df = pd.concat([training_df.reset_index(drop=True), embedding_df], axis=1)\n",
    "\n",
    "# Display the final DataFrame with embeddings\n",
    "training_df.head()"
   ],
   "id": "302f587eb9630970",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(list(training_df.columns))",
   "id": "2e64ba8291222867",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train the models",
   "id": "f1b023343e4e7847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from backend.models import train_model, evaluate_model, plot_predictions, prepare_data, objective\n",
    "import optuna\n",
    "\n",
    "# Reduce logging output (only show errors)\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# Define the prediction horizons\n",
    "days_to_predict = [1, 7, 14, 21, 28]"
   ],
   "id": "c0d3d4ad67759a6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = prepare_data(training_df, days_to_predict)"
   ],
   "id": "a3e7c3d39bbed8c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Number of trials\n",
    "n_trials = 1000\n",
    "n_warmup_steps = 25\n",
    "\n",
    "# Initialize progress bars\n",
    "tqdm_ridge = tqdm(total=n_trials, desc=\"Optimizing Ridge (Best R²: -∞)\")\n",
    "tqdm_svr = tqdm(total=n_trials, desc=\"Optimizing SVR (Best R²: -∞)\")\n",
    "\n",
    "# Track best R² scores\n",
    "best_ridge_r2 = float('-inf')\n",
    "best_svr_r2 = float('-inf')\n",
    "\n",
    "\n",
    "def tqdm_callback_ridge(study, trial):\n",
    "    \"\"\"Update tqdm progress bar and show best R² score for Ridge.\"\"\"\n",
    "    global best_ridge_r2\n",
    "    if study.best_trial.value > best_ridge_r2:\n",
    "        best_ridge_r2 = study.best_trial.value\n",
    "    tqdm_ridge.set_description(f\"Optimizing Ridge (Best R²: {best_ridge_r2:.4f})\")\n",
    "    tqdm_ridge.update(1)\n",
    "\n",
    "\n",
    "def tqdm_callback_svr(study, trial):\n",
    "    \"\"\"Update tqdm progress bar and show best R² score for SVR.\"\"\"\n",
    "    global best_svr_r2\n",
    "    if study.best_trial.value > best_svr_r2:\n",
    "        best_svr_r2 = study.best_trial.value\n",
    "    tqdm_svr.set_description(f\"Optimizing SVR (Best R²: {best_svr_r2:.4f})\")\n",
    "    tqdm_svr.update(1)\n",
    "\n",
    "\n",
    "# Optimize Ridge\n",
    "study_ridge = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(),\n",
    "                                  pruner=optuna.pruners.MedianPruner(n_warmup_steps=n_warmup_steps))\n",
    "study_ridge.optimize(lambda trial: objective(trial, \"Ridge\", X_train, y_train, X_test, y_test),\n",
    "                     n_trials=n_trials, callbacks=[tqdm_callback_ridge])\n",
    "\n",
    "# Optimize SVR\n",
    "study_svr = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(),\n",
    "                                pruner=optuna.pruners.MedianPruner(n_warmup_steps=n_warmup_steps))\n",
    "study_svr.optimize(lambda trial: objective(trial, \"SVR\", X_train, y_train, X_test, y_test),\n",
    "                   n_trials=n_trials, callbacks=[tqdm_callback_svr])\n",
    "\n",
    "# Close progress bars\n",
    "tqdm_ridge.close()\n",
    "tqdm_svr.close()\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest Ridge Parameters:\")\n",
    "print(study_ridge.best_params)\n",
    "\n",
    "print(\"\\nBest SVR Parameters:\")\n",
    "print(study_svr.best_params)"
   ],
   "id": "d1f870486457a421",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use best parameters from Optuna\n",
    "best_ridge_params = study_ridge.best_params\n",
    "best_svr_params = study_svr.best_params\n",
    "\n",
    "# Train with best parameters\n",
    "models = {\n",
    "    \"LinearRegression\": {},\n",
    "    \"Ridge\": best_ridge_params,\n",
    "    \"SVR\": best_svr_params,\n",
    "}\n",
    "\n",
    "all_results = {}\n",
    "for model_name, params in models.items():\n",
    "    print(f\"\\nTraining {model_name} with Optimized Parameters...\\n\" + \"=\" * 50)\n",
    "\n",
    "    # Train model\n",
    "    model, y_pred = train_model(model_name, X_train, y_train, X_test, **params)\n",
    "\n",
    "    # Evaluate model\n",
    "    results = evaluate_model(model_name, y_test, y_pred, days_to_predict)\n",
    "\n",
    "    # Store results\n",
    "    all_results[model_name] = results\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_predictions(y_test, y_pred, days_to_predict, results, model_name)"
   ],
   "id": "e034a6584b34fbc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "577b2b88",
   "metadata": {
    "papermill": {
     "duration": 0.0286,
     "end_time": "2024-12-20T22:32:38.900671",
     "exception": false,
     "start_time": "2024-12-20T22:32:38.872071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion and Future Work\n",
    "..."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f7bb6e4d40bbcf75",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "34-Year Daily Stock Data Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.964457,
   "end_time": "2024-12-20T22:32:39.675025",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-20T22:32:32.710568",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
