{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "\n",
    "from backend.autoencoders import Autoencoder, train_predict_autoencoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "640a2e7cf6e8d37a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b18855ea",
   "metadata": {
    "papermill": {
     "duration": 0.003065,
     "end_time": "2024-12-20T22:32:34.882291",
     "exception": false,
     "start_time": "2024-12-20T22:32:34.879226",
     "status": "completed"
    },
    "tags": []
   },
   "source": "# Data Loading"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Load a DataFrame with a specific version of a CSV\n",
    "df: pd.DataFrame = kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"shiveshprakash/34-year-daily-stock-data/versions/1\",\n",
    "    \"stock_data.csv\",\n",
    ")\n",
    "\n",
    "# Drop useless columns or which we will create ourselves\n",
    "df = df.drop(columns=[\"prev_day\"])\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ],
   "id": "88b04371e8b9dd55",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "271de872",
   "metadata": {
    "papermill": {
     "duration": 0.003422,
     "end_time": "2024-12-20T22:32:36.896597",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.893175",
     "status": "completed"
    },
    "tags": []
   },
   "source": "# Data Cleaning and Preprocessing"
  },
  {
   "cell_type": "code",
   "id": "32f8eb5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:36.905286Z",
     "iopub.status.busy": "2024-12-20T22:32:36.904900Z",
     "iopub.status.idle": "2024-12-20T22:32:36.915093Z",
     "shell.execute_reply": "2024-12-20T22:32:36.914061Z"
    },
    "id": "data-cleaning",
    "papermill": {
     "duration": 0.016478,
     "end_time": "2024-12-20T22:32:36.916812",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.900334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e055ca0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:36.925869Z",
     "iopub.status.busy": "2024-12-20T22:32:36.925530Z",
     "iopub.status.idle": "2024-12-20T22:32:36.938118Z",
     "shell.execute_reply": "2024-12-20T22:32:36.937196Z"
    },
    "id": "data-preprocessing",
    "papermill": {
     "duration": 0.019021,
     "end_time": "2024-12-20T22:32:36.939800",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.920779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convert \"dt\" to datetime format\n",
    "df[\"dt\"] = pd.to_datetime(df[\"dt\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Check data types\n",
    "df.dtypes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa788377",
   "metadata": {
    "papermill": {
     "duration": 0.003836,
     "end_time": "2024-12-20T22:32:36.947821",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.943985",
     "status": "completed"
    },
    "tags": []
   },
   "source": "# Data Analysis"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### General Plots",
   "id": "25cc0f0cd6045363"
  },
  {
   "cell_type": "code",
   "id": "15b99a7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:36.957262Z",
     "iopub.status.busy": "2024-12-20T22:32:36.956917Z",
     "iopub.status.idle": "2024-12-20T22:32:37.395839Z",
     "shell.execute_reply": "2024-12-20T22:32:37.394832Z"
    },
    "id": "eda",
    "papermill": {
     "duration": 0.445954,
     "end_time": "2024-12-20T22:32:37.397827",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.951873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plot the S&P 500 over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=df[\"dt\"], y=df[\"sp500\"], label=\"S&P 500\")\n",
    "plt.title(\"S&P 500 Index Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"S&P 500 Index\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f06cc25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:37.409048Z",
     "iopub.status.busy": "2024-12-20T22:32:37.408621Z",
     "iopub.status.idle": "2024-12-20T22:32:37.690464Z",
     "shell.execute_reply": "2024-12-20T22:32:37.689398Z"
    },
    "id": "eda-2",
    "papermill": {
     "duration": 0.289506,
     "end_time": "2024-12-20T22:32:37.692271",
     "exception": false,
     "start_time": "2024-12-20T22:32:37.402765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualize the relationship between S&P 500 and DJIA\n",
    "sns.scatterplot(x=\"sp500\", y=\"djia\", data=df)\n",
    "plt.title(\"S&P 500 vs DJIA\")\n",
    "plt.xlabel(\"S&P 500 Index\")\n",
    "plt.ylabel(\"DJIA Index\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab5b2c71",
   "metadata": {
    "papermill": {
     "duration": 0.00554,
     "end_time": "2024-12-20T22:32:37.705110",
     "exception": false,
     "start_time": "2024-12-20T22:32:37.699570",
     "status": "completed"
    },
    "tags": []
   },
   "source": "### Correlation Analysis"
  },
  {
   "cell_type": "code",
   "id": "1651eb6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:37.718150Z",
     "iopub.status.busy": "2024-12-20T22:32:37.717809Z",
     "iopub.status.idle": "2024-12-20T22:32:38.385552Z",
     "shell.execute_reply": "2024-12-20T22:32:38.384335Z"
    },
    "id": "correlation-analysis",
    "papermill": {
     "duration": 0.677953,
     "end_time": "2024-12-20T22:32:38.389005",
     "exception": false,
     "start_time": "2024-12-20T22:32:37.711052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Select only numeric columns for correlation analysis\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create new features",
   "id": "3eec01b82184f2f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Aggregate Rolling Features",
   "id": "94ab71317a25c398"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "windows = [7, 14, 30, 90]  # Rolling window sizes (days)\n",
    "\n",
    "for window in windows:\n",
    "    df[f\"sp500_mean_{window}\"] = df[\"sp500\"].rolling(window=window).mean()\n",
    "    df[f\"sp500_std_{window}\"] = df[\"sp500\"].rolling(window=window).std()\n",
    "    df[f\"vix_mean_{window}\"] = df[\"vix\"].rolling(window=window).mean()\n",
    "    df[f\"sp500_volume_mean_{window}\"] = df[\"sp500_volume\"].rolling(window=window).mean()\n",
    "    df[f\"djia_mean_{window}\"] = df[\"djia\"].rolling(window=window).mean()\n",
    "\n",
    "# Drop rows with NaN values introduced by rolling calculations\n",
    "df = df.dropna()\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()"
   ],
   "id": "6bd47f79caf76ccd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fourier Transform",
   "id": "cf6879bc558cef02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fourier Transform for S&P 500\n",
    "N = len(df[\"sp500\"])\n",
    "T = 1  # Time step (1 day)\n",
    "yf = rfft(df[\"sp500\"])  # Fourier Transform\n",
    "xf = rfftfreq(N, T)  # Frequencies\n",
    "\n",
    "# Define frequency ranges for specific windows\n",
    "freq_ranges = {\n",
    "    \"daily_to_weekly\": (1 / 7, 1 / 1),  # 1 to 7 days\n",
    "    \"weekly_to_monthly\": (1 / 30, 1 / 7),  # 7 to 30 days\n",
    "    \"monthly_to_quarterly\": (1 / 90, 1 / 30),  # 30 to 90 days\n",
    "}\n",
    "\n",
    "# Extract magnitudes for each range\n",
    "for key, (low, high) in freq_ranges.items():\n",
    "    mask = (xf >= low) & (xf <= high)\n",
    "    df[f\"sp500_fft_{key}_mean\"] = np.mean(np.abs(yf[mask]))\n",
    "    df[f\"sp500_fft_{key}_max\"] = np.max(np.abs(yf[mask]))\n",
    "    df[f\"sp500_fft_{key}_sum\"] = np.sum(np.abs(yf[mask]))\n",
    "\n",
    "# Repeat the process for VIX\n",
    "yf_vix = rfft(df[\"vix\"])\n",
    "for key, (low, high) in freq_ranges.items():\n",
    "    mask = (xf >= low) & (xf <= high)\n",
    "    df[f\"vix_fft_{key}_mean\"] = np.mean(np.abs(yf_vix[mask]))\n",
    "    df[f\"vix_fft_{key}_max\"] = np.max(np.abs(yf_vix[mask]))\n",
    "    df[f\"vix_fft_{key}_sum\"] = np.sum(np.abs(yf_vix[mask]))\n",
    "\n",
    "# Drop irrelevant data\n",
    "df = df.dropna()\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()"
   ],
   "id": "5a41c8a6a07699ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Autoencoders",
   "id": "21535c3d73c557af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Create lagged features (temporary DataFrame)\n",
    "lag_days = 30\n",
    "lagged_df = pd.DataFrame()\n",
    "for lag in range(1, lag_days + 1):\n",
    "    lagged_df[f\"sp500_lag_{lag}\"] = df[\"sp500\"].shift(lag)\n",
    "    lagged_df[f\"vix_lag_{lag}\"] = df[\"vix\"].shift(lag)\n",
    "    lagged_df[f\"sp500_volume_lag_{lag}\"] = df[\"sp500_volume\"].shift(lag)\n",
    "\n",
    "# Step 2: Drop rows with NaN values\n",
    "lagged_df = lagged_df.dropna()\n",
    "df = df.iloc[lag_days:].reset_index(drop=True)\n",
    "\n",
    "# Step 3: Normalize lagged features\n",
    "scaler = MinMaxScaler()\n",
    "X_lagged = scaler.fit_transform(lagged_df.values)\n",
    "joblib.dump(scaler, \"scaler.pkl\")  # Save the scaler\n",
    "\n",
    "# Step 4: Get input dimensions\n",
    "input_dim = X_lagged.shape[1]  # Number of lagged features\n",
    "\n",
    "# Display the number of lagged features\n",
    "print(f\"Nr of lagged features: {input_dim}\")"
   ],
   "id": "72146f3e514985e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the Autoencoder\n",
    "encoding_dim = 10  # Compressed representation size\n",
    "autoencoder = Autoencoder(input_dim, encoding_dim)\n",
    "\n",
    "# Train the autoencoder and get embeddings\n",
    "trained_autoencoder, embeddings = train_predict_autoencoder(\n",
    "    autoencoder,\n",
    "    X_lagged,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    lr=0.0005,\n",
    "    l1_penalty=0.001,\n",
    "    weight_decay=1e-5\n",
    ")"
   ],
   "id": "b37ef7f2808347bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert embeddings to DataFrame\n",
    "embedding_df = pd.DataFrame(embeddings, columns=[f\"embed_{i + 1}\" for i in range(embeddings.shape[1])])\n",
    "\n",
    "# Attach embeddings to the main DataFrame\n",
    "df = pd.concat([df.reset_index(drop=True), embedding_df], axis=1)\n",
    "\n",
    "# Clean up: Delete the temporary lagged features DataFrame\n",
    "del lagged_df\n",
    "\n",
    "# Display the final DataFrame with embeddings\n",
    "df.head()"
   ],
   "id": "302f587eb9630970",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train the models",
   "id": "f1b023343e4e7847"
  },
  {
   "cell_type": "code",
   "id": "5d082cd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:38.424834Z",
     "iopub.status.busy": "2024-12-20T22:32:38.424353Z",
     "iopub.status.idle": "2024-12-20T22:32:38.836542Z",
     "shell.execute_reply": "2024-12-20T22:32:38.833655Z"
    },
    "id": "predictive-modeling",
    "papermill": {
     "duration": 0.427977,
     "end_time": "2024-12-20T22:32:38.842592",
     "exception": false,
     "start_time": "2024-12-20T22:32:38.414615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define features and target\n",
    "X = numeric_df.drop(columns=[\"sp500\"])\n",
    "y = numeric_df[\"sp500\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse, r2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "577b2b88",
   "metadata": {
    "papermill": {
     "duration": 0.0286,
     "end_time": "2024-12-20T22:32:38.900671",
     "exception": false,
     "start_time": "2024-12-20T22:32:38.872071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion and Future Work\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "34-Year Daily Stock Data Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.964457,
   "end_time": "2024-12-20T22:32:39.675025",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-20T22:32:32.710568",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
