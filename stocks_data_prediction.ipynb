{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from backend.autoencoders import Autoencoder, train_predict_autoencoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "640a2e7cf6e8d37a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b18855ea",
   "metadata": {
    "papermill": {
     "duration": 0.003065,
     "end_time": "2024-12-20T22:32:34.882291",
     "exception": false,
     "start_time": "2024-12-20T22:32:34.879226",
     "status": "completed"
    },
    "tags": []
   },
   "source": "# Data Loading"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Load a DataFrame with a specific version of a CSV\n",
    "df: pd.DataFrame = kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"shiveshprakash/34-year-daily-stock-data/versions/1\",\n",
    "    \"stock_data.csv\",\n",
    ")\n",
    "\n",
    "# Drop useless columns or which we will create ourselves\n",
    "df = df.drop(columns=[\"prev_day\"])\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ],
   "id": "88b04371e8b9dd55",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "271de872",
   "metadata": {
    "papermill": {
     "duration": 0.003422,
     "end_time": "2024-12-20T22:32:36.896597",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.893175",
     "status": "completed"
    },
    "tags": []
   },
   "source": "# Data Cleaning and Preprocessing"
  },
  {
   "cell_type": "code",
   "id": "32f8eb5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:36.905286Z",
     "iopub.status.busy": "2024-12-20T22:32:36.904900Z",
     "iopub.status.idle": "2024-12-20T22:32:36.915093Z",
     "shell.execute_reply": "2024-12-20T22:32:36.914061Z"
    },
    "id": "data-cleaning",
    "papermill": {
     "duration": 0.016478,
     "end_time": "2024-12-20T22:32:36.916812",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.900334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e055ca0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:36.925869Z",
     "iopub.status.busy": "2024-12-20T22:32:36.925530Z",
     "iopub.status.idle": "2024-12-20T22:32:36.938118Z",
     "shell.execute_reply": "2024-12-20T22:32:36.937196Z"
    },
    "id": "data-preprocessing",
    "papermill": {
     "duration": 0.019021,
     "end_time": "2024-12-20T22:32:36.939800",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.920779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convert \"dt\" to datetime format\n",
    "df[\"dt\"] = pd.to_datetime(df[\"dt\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Check data types\n",
    "df.dtypes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa788377",
   "metadata": {
    "papermill": {
     "duration": 0.003836,
     "end_time": "2024-12-20T22:32:36.947821",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.943985",
     "status": "completed"
    },
    "tags": []
   },
   "source": "# Data Analysis"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### General Plots",
   "id": "25cc0f0cd6045363"
  },
  {
   "cell_type": "code",
   "id": "15b99a7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:36.957262Z",
     "iopub.status.busy": "2024-12-20T22:32:36.956917Z",
     "iopub.status.idle": "2024-12-20T22:32:37.395839Z",
     "shell.execute_reply": "2024-12-20T22:32:37.394832Z"
    },
    "id": "eda",
    "papermill": {
     "duration": 0.445954,
     "end_time": "2024-12-20T22:32:37.397827",
     "exception": false,
     "start_time": "2024-12-20T22:32:36.951873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plot the S&P 500 over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=df[\"dt\"], y=df[\"sp500\"], label=\"S&P 500\")\n",
    "plt.title(\"S&P 500 Index Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"S&P 500 Index\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f06cc25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:37.409048Z",
     "iopub.status.busy": "2024-12-20T22:32:37.408621Z",
     "iopub.status.idle": "2024-12-20T22:32:37.690464Z",
     "shell.execute_reply": "2024-12-20T22:32:37.689398Z"
    },
    "id": "eda-2",
    "papermill": {
     "duration": 0.289506,
     "end_time": "2024-12-20T22:32:37.692271",
     "exception": false,
     "start_time": "2024-12-20T22:32:37.402765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualize the relationship between S&P 500 and DJIA\n",
    "sns.scatterplot(x=\"sp500\", y=\"djia\", data=df)\n",
    "plt.title(\"S&P 500 vs DJIA\")\n",
    "plt.xlabel(\"S&P 500 Index\")\n",
    "plt.ylabel(\"DJIA Index\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab5b2c71",
   "metadata": {
    "papermill": {
     "duration": 0.00554,
     "end_time": "2024-12-20T22:32:37.705110",
     "exception": false,
     "start_time": "2024-12-20T22:32:37.699570",
     "status": "completed"
    },
    "tags": []
   },
   "source": "### Correlation Analysis"
  },
  {
   "cell_type": "code",
   "id": "1651eb6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T22:32:37.718150Z",
     "iopub.status.busy": "2024-12-20T22:32:37.717809Z",
     "iopub.status.idle": "2024-12-20T22:32:38.385552Z",
     "shell.execute_reply": "2024-12-20T22:32:38.384335Z"
    },
    "id": "correlation-analysis",
    "papermill": {
     "duration": 0.677953,
     "end_time": "2024-12-20T22:32:38.389005",
     "exception": false,
     "start_time": "2024-12-20T22:32:37.711052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Select only numeric columns for correlation analysis\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create new features",
   "id": "3eec01b82184f2f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Aggregate Rolling Features",
   "id": "94ab71317a25c398"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "windows = [7, 14, 30, 90, 365]  # Rolling window sizes (days)\n",
    "\n",
    "for window in windows:\n",
    "    df[f\"sp500_mean_{window}\"] = df[\"sp500\"].rolling(window=window).mean()\n",
    "    df[f\"sp500_std_{window}\"] = df[\"sp500\"].rolling(window=window).std()\n",
    "    df[f\"sp500_volume_mean_{window}\"] = df[\"sp500_volume\"].rolling(window=window).mean()\n",
    "    df[f\"sp500_volume_std_{window}\"] = df[\"sp500_volume\"].rolling(window=window).std()\n",
    "\n",
    "    df[f\"djia_mean_{window}\"] = df[\"djia\"].rolling(window=window).mean()\n",
    "    df[f\"djia_std_{window}\"] = df[\"djia\"].rolling(window=window).std()\n",
    "    df[f\"djia_volume_mean_{window}\"] = df[\"djia_volume\"].rolling(window=window).mean()\n",
    "    df[f\"djia_volume_std_{window}\"] = df[\"djia_volume\"].rolling(window=window).std()\n",
    "\n",
    "    df[f\"hsi_mean_{window}\"] = df[\"hsi\"].rolling(window=window).mean()\n",
    "    df[f\"hsi_std_{window}\"] = df[\"hsi\"].rolling(window=window).std()\n",
    "\n",
    "    df[f\"vix_mean_{window}\"] = df[\"vix\"].rolling(window=window).mean()\n",
    "    df[f\"vix_std_{window}\"] = df[\"vix\"].rolling(window=window).std()\n",
    "\n",
    "# Drop rows with NaN values introduced by rolling calculations\n",
    "df = df.dropna()\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()"
   ],
   "id": "6bd47f79caf76ccd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Autoencoders",
   "id": "21535c3d73c557af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create lagged features (temporary DataFrame)\n",
    "lag_days = 365\n",
    "lagged_df = pd.DataFrame()\n",
    "for lag in range(1, lag_days + 1):\n",
    "    lagged_df[f\"sp500_lag_{lag}\"] = df[\"sp500\"].shift(lag)\n",
    "    lagged_df[f\"vix_lag_{lag}\"] = df[\"vix\"].shift(lag)\n",
    "    lagged_df[f\"sp500_volume_lag_{lag}\"] = df[\"sp500_volume\"].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "lagged_df = lagged_df.dropna()\n",
    "df = df.iloc[lag_days:].reset_index(drop=True)\n",
    "\n",
    "# Normalize lagged features\n",
    "scaler = MinMaxScaler()\n",
    "X_lagged = scaler.fit_transform(lagged_df.values)\n",
    "joblib.dump(scaler, \"lagged_scaler.pkl\")  # Save the scaler\n",
    "\n",
    "# Clean up: Delete the temporary lagged features DataFrame\n",
    "del lagged_df\n",
    "\n",
    "# Get input dimensions\n",
    "input_dim = X_lagged.shape[1]  # Number of lagged features\n",
    "\n",
    "# Display the number of lagged features\n",
    "print(f\"Nr of lagged features: {input_dim}\")"
   ],
   "id": "72146f3e514985e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the Autoencoder\n",
    "encoding_dim = 10  # Compressed representation size\n",
    "autoencoder = Autoencoder(input_dim, encoding_dim)\n",
    "\n",
    "# Train the autoencoder and get embeddings\n",
    "trained_autoencoder, embeddings = train_predict_autoencoder(\n",
    "    autoencoder,\n",
    "    X_lagged,\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    lr=0.0005,\n",
    "    l1_penalty=0.001,\n",
    "    weight_decay=1e-5\n",
    ")"
   ],
   "id": "b37ef7f2808347bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert embeddings to DataFrame\n",
    "embedding_df = pd.DataFrame(embeddings, columns=[f\"embed_{i + 1}\" for i in range(embeddings.shape[1])])"
   ],
   "id": "55e953d8f57ab8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare the final df to train",
   "id": "13f6e0affe261b19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Take the right data for training",
   "id": "3df0dd01ab1a4b87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select only numeric columns for training\n",
    "training_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Convert to an ordered categorical column\n",
    "if training_df[\"joblessness\"].dtypes != \"category\":\n",
    "    training_df[\"joblessness\"] = pd.Categorical(\n",
    "        training_df[\"joblessness\"],\n",
    "        categories=[1, 2, 3, 4],\n",
    "        ordered=True\n",
    "    )"
   ],
   "id": "2c360326d9e64aa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Scale the df",
   "id": "b93e6935118fab42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Separate the \"joblessness\" column\n",
    "joblessness = training_df[\"joblessness\"]\n",
    "\n",
    "# Select all columns except \"joblessness\"\n",
    "columns_to_scale = training_df.drop(columns=[\"joblessness\"]).columns\n",
    "\n",
    "# Apply MinMaxScaler to the selected columns\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(training_df[columns_to_scale])\n",
    "joblib.dump(scaler, \"training_df_scaler.pkl\")  # Save the scaler\n",
    "\n",
    "# Create a DataFrame for the scaled data\n",
    "training_df = pd.DataFrame(scaled_data, columns=columns_to_scale, index=training_df.index)\n",
    "\n",
    "# Add back the \"joblessness\" column\n",
    "training_df[\"joblessness\"] = joblessness"
   ],
   "id": "44043096e505e76b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Attach embeddings to the training DataFrame",
   "id": "4a63be8a5c808bb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Attach embeddings to the main DataFrame\n",
    "training_df = pd.concat([training_df.reset_index(drop=True), embedding_df], axis=1)\n",
    "\n",
    "# Display the final DataFrame with embeddings\n",
    "training_df.head()"
   ],
   "id": "302f587eb9630970",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(list(training_df.columns))",
   "id": "2e64ba8291222867",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train the models",
   "id": "f1b023343e4e7847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "days_to_predict = [1, 7, 14, 21, 28]"
   ],
   "id": "c0d3d4ad67759a6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 1: Create targets for the selected future days\n",
    "next_days_columns = []\n",
    "for day in days_to_predict:\n",
    "    next_day_col = f\"sp500_next_{day}\"\n",
    "    training_df[next_day_col] = training_df[\"sp500\"].shift(-day)\n",
    "    next_days_columns.append(next_day_col)\n",
    "\n",
    "# Drop rows where any target is NaN (due to shifting)\n",
    "training_df = training_df.dropna()\n",
    "\n",
    "# Step 2: Define features (X) and multiple targets (y)\n",
    "X = training_df.drop(columns=next_days_columns)\n",
    "y = training_df[next_days_columns]\n",
    "\n",
    "# Step 3: Perform time-based train-test split\n",
    "split_index = int(0.8 * len(X))  # 80% for training\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Step 4: Train the model (multi-output regression)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions for the selected days\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model for each selected future day and store results\n",
    "evaluation_results = {}  # Dictionary to store MSE and R² for each day\n",
    "\n",
    "for i, day in enumerate(days_to_predict):\n",
    "    # Align the predictions by shifting y_pred\n",
    "    y_pred_aligned = pd.Series(y_pred[:, i], index=y_test.index).shift(-day).dropna()\n",
    "    y_test_aligned = y_test.iloc[:-day, i]  # Drop last 'day' rows in y_test to align with predictions\n",
    "\n",
    "    # Ensure both are the same length for evaluation\n",
    "    assert len(y_pred_aligned) == len(y_test_aligned), f\"Mismatch for Day {day}\"\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test_aligned, y_pred_aligned)\n",
    "    r2 = r2_score(y_test_aligned, y_pred_aligned)\n",
    "\n",
    "    # Store results\n",
    "    evaluation_results[day] = {\"MSE\": mse, \"R²\": r2}\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Day {day}: Mean Squared Error: {mse:.6f}, R² Score: {r2:.6f}\")"
   ],
   "id": "14fb01a840ddea48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot predictions vs actual values for each selected day with metrics\n",
    "for i, day in enumerate(days_to_predict):  # Loop through specified prediction horizons\n",
    "    # Align the predictions by shifting y_pred\n",
    "    y_pred_aligned = pd.Series(y_pred[:, i], index=y_test.index).shift(-day).dropna()\n",
    "\n",
    "    # Extract stored metrics\n",
    "    mse = evaluation_results[day][\"MSE\"]\n",
    "    r2 = evaluation_results[day][\"R²\"]\n",
    "\n",
    "    # Plot for the specific day\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test.iloc[:, i].values, label=f\"Actual Day {day}\", alpha=0.8)\n",
    "    plt.plot(y_pred_aligned.values, label=f\"Predicted Day {day}\", linestyle=\"--\", alpha=0.8)\n",
    "\n",
    "    # Add text box with MSE and R²\n",
    "    textstr = f\"MSE: {mse:.4f}\\n    R²: {r2:.4f}\"\n",
    "    plt.text(0.05, 0.85, textstr, transform=plt.gca().transAxes,\n",
    "             fontsize=12, verticalalignment='top',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"))\n",
    "\n",
    "    plt.title(f\"Actual vs Predicted for Day {day}\")\n",
    "    plt.xlabel(\"Test Samples\")\n",
    "    plt.ylabel(\"S&P 500 Index\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ],
   "id": "4c45c9ae57160c84",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "577b2b88",
   "metadata": {
    "papermill": {
     "duration": 0.0286,
     "end_time": "2024-12-20T22:32:38.900671",
     "exception": false,
     "start_time": "2024-12-20T22:32:38.872071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion and Future Work\n",
    "..."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f7bb6e4d40bbcf75",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "34-Year Daily Stock Data Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.964457,
   "end_time": "2024-12-20T22:32:39.675025",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-20T22:32:32.710568",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
